
\citet{bowers2013sutva} showed that one could use Fisher's
randomization-based hypothesis testing framework to assess counterfactual
causal models of treatment propagation and spillover across social networks.
That is, they showed that one could make
statements such as: ``Our design provides strong evidence against the idea
that the treatment spilled over by more than 5 points given this model of
treatment propagation.'' This research note shows that the test statistic that
they used --- the Kolmogorov-Smirnov test statistic  \citep[\S
5.4]{MylesHollander1999a} --- could be improved substantially. We demonstrate that the sum of squared residuals from a linear
model in which aspects of the network play a role is a preferable test
statistic for the model and network and design analyzed by
\citet{bowers2013sutva} (henceforth BFP). The point of this short article is to improve the
use of the ``reasoning about interference'' approach and also to highlight
the methodological and statistical questions raised by that article. We do
not aim for general answers to those questions here and now, but we do hope
that this piece stimulates further, and  deeper, work on test statistics and
sharp hypotheses.

\section{Background}

We can define a counterfactual causal effect for a person $i$ as a difference
between the outcome, $y_i$, person $i$ would have expressed
after a set of treatments, collected in the vector $\bz$, had been assigned to some subset of a social network,
$y_{i,\bz}$ compared to the outcome that person $i$ would have expressed
under a different treatment vector. If the experiment had an effect on person
$i$, then her outcome would differ under different realizations of the
experimental treatment $y_{i,\bz} \ne y_{i,\bz'}$. The fundamental problem of
causal inference (a phrase famously coined in \cite{holland:1986a}) reminds
us that we can never see both states of the world: we only observe the
outcome from person $i$ under one treatment assignment, either $\bz$ or
$\bz'$ not both.

R.A. Fisher's approach to design-based statistical inference  \citep[Chap
2]{fisher:1935} begins with the idea that we do not learn directly about the
unobserved counterfactuals. Rather, he suggests, we can learn about models of
unobserved counterfactuals --- we can learn how much information we have to
dispel certain claims about those counterfactuals as implausible. This
conceptual move --- sidestepping the fundamental problem of causal inference
via learning about claims made by scientists --- drives hypothesis testing in
general.  BFP build on this insight as developed and
linked to counterfactual causal inference by Paul Rosenbaum
\citep{rosenbaum2010design}. They also say: we do not know how the
counterfactual conditions might have turned out, they are unobserved, but we
can make claims about how treatment might have come to have an effect and we
can assess those claims. For example, the claim used by
BFP is a model in which the effects of treatment
propagating from one person to another die off as the network distance between
nodes increases.\footnote{See the paper itself for details of the example
  model.} They further showed that the strength of evidence against the
specific hypotheses implied by a given model varied with different features of
the design as well as the extent to which the true process by which treatment
given to one unit changed the outcomes of another unit diverged from the
model. Throughout their paper they used the Kolmogorov-Smirnov (KS) test
statistic so that their tests would be sensitive differences in the treatment
and control distributions implied by different hypotheses and not merely
sensitive to differences in one aspect of those distributions (such as the
differences in the mean).\footnote{If the empirical cumulative distribution
  function (ECDF) of the treated units is $F_1$ and the ECDF of the control
  units is $F_0$ then the KS test statistic is $\T(\yu,\bz) = \underset{i =
    1,\ldots,n}{\text{max}}\left[F_1(y_{i,\bzero}) -
    F_0(y_{i,\bzero})\right]$, where $F(x)=(1/n)\sum_{i=1}^n I(x_i \le x)$
  records the proportion of the distribution of $x$ at or below $x_i$
  \citep[\S 5.4]{MylesHollander1999a}.}

Notice that the test statistic choice matters: the engine of statistical
inference involves summarizing information against the hypothesized claim, yet
different ways to summarize information might be more or less sensitive to
substantively meaningful differences. A simple test of the
sharp null can be executed with power that varies as a function of the design
of the study (proportion treated, blocking structure, etc..), characteristics
of the outcome, and the way that a test statistic summarizes the outcome. Yet,
when the models are complex,  BFP showed that sometimes a
test will have (1) no power to address the model at all such that all hypothesized parameters
would receive the same implausibility assessment; (2) or might reject all such
hypothesized parameters. Thus, although in theory one may assess sharp
hypotheses about multiparameter settings, in practice one may not learn much
from such tests --- or, in the absence of simulation studies assessing the
model and test statistic, one may be mislead.

How should one choose a test statistic to assess relationships between complex
models of treatment propagation and data? General advice for test statistics
is that they should be powerful against relevant alternatives (find canonical
cite). \citet[\S 2.4.4]{rosenbaum:2002} provides more specific advice about
large sample performance of certain classes of test statistics and BFP repeat
his general advice: `` Select a test statistic T that will be small when the
treated and control distributions in the adjusted data \ldots are
similar, and large when the distributions diverge.'' \citep[p.
107]{bowers2013sutva}. They should have noted, instead, that when models
involve more than one parameter, such as the models of treatment propagation across
networks, test statistics should be increasing in \emph{all} of the parameters
(for example, in both parameters capturing spillover and parameters capturing
direct experience with the treatment). Here, we also show a benefit from
taking into account the network structure in the test statistic as well as in
the model. 

We demonstrate two results, each showing how effect increasing test statistics,
as defined by BFP, may provide little to no information about the spillover
parameter $\tau_2$.  Let us first consider the case when the test statistic is
the absolute mean difference between hypothesized uniformity trial outcomes
for treated and control individuals: $\T(\widetilde y_{ij}(\0,\Z), \Z') =
\left\vert \mu_1 ( \widetilde y_{ij}(\0,\Z), \Z' ) - \mu_0 ( \widetilde
  y_{ij}(\0,\Z), \Z' ) \right\vert.  $ Fix a realization of $\Z = \z$, and
further assume noncollinearity: $ \z \notin \{\z^I, (1-\z^I)\}$. We can see
that, for any hypothesized $\widetilde\tau_2$, there always exists a
$\widetilde\tau_1$ such that the $p$-value is 1. Expanding the test statistic,

\begin{flalign}
  \T(\widetilde y_{ij}(\0,\z), \z) = & \left\vert \mu_1 (y_{ij}(\z) - \widetilde\tau_1 - \tau_2 z_{i(-j)}, \z) - \mu_0(\widetilde y_{ij}(\z) -
  \widetilde\tau_2 z_{i(-j)} , \z) \right\vert \\
= & \left\vert \mu_1 (y_{ij}(\z)
  - \widetilde\tau_2 z_{i(-j)}, \z)- \mu_0 (\widetilde y_{ij}(\z) -
  \widetilde\tau_2 z_{i(-j)}, \z)- \widetilde\tau_1 \right\vert.
\end{flalign}

Thus,
regardless of the value of $\widetilde\tau_2$, we can always find a
$\widetilde\tau_1$ that sets $\T(\widetilde y_{ij}(\0,\z), \z) = 0$. Given
that $\T(\widetilde y_{ij}(\0,\z), \Z')$ can only take values in
$\mathbb{R}^+$, then $p=1$, as any other possible randomization $\Z'$ will
yield a value of $\T(\widetilde y_{ij}(\0,\z), \Z') \geq 0$. It follows that,
given this test, we cannot rule out any value of $\tau_2$ as being
implausible.\footnote{In fact, this result does not depend on example in the
  setting, and holds for any randomization scheme coupled with a linear causal
  model of the form \eqref{causal}.}

Second, I show that, given the running example, a test based on any effect
increasing statistic is inconsistent against a radically incorrect hypothesis
about $\tau_2$. Let us assume that $\tau_1$ is properly hypothesized at its
true value, and that the hypothesized value of $\tau_2$ ($\widetilde \tau_2$)
is radically incorrect: $\vert \widetilde\tau_2 - \tau_2 \vert \gg
\max(y_{ij}(\0)) - \min(y_{ij}(\0))$. Denote $\beta \equiv \tau_2 -
\widetilde\tau_2$, and assume $\vert \beta \vert < \infty$.  Under the
assumption that $\tau_2 = \widetilde \tau_2$: $\widetilde y_{ij}(\0,\Z) =
y_{ij}(\0) + \beta Z_{i(-j)}$.  Assume a sequence of nested finite populations
$\{U_k\} = U_1, U_2,...$, growing in $k$ households, so that $U_k$ consists of
$N_k = 2k$ individuals, each with an associated random assignment $\Z_k$
\citep{isaki1982survey}. (For notational parsimony, I drop explicit dependence
on the subscript $k$.)  Given an incorrect hypothesis, a necessary, but not
sufficient, condition for a test's consistency is that $\lim_{k \rightarrow
\infty} \Pr(p > \alpha) < \epsilon$, for any $\alpha,\epsilon > 0$.

Let an effect increasing test statistic $\T(\widetilde y_{ij}(\0,\Z),\Z')$ be
some measure of the absolute divergence in empirical distributions of
$\widetilde y_{ij}(\0,\Z)$ for treated and control individuals under the
treatment vector $\Z'$.  Since distributional differences in $y_{ij}(\0)$ are
small relative to shifts induced by differences in proportions of treated
housemates,  $\T(\widetilde y_{ij}(\0,\Z),\Z')$ will be increasing in $
\left\vert \mu_1(Z_{i(-j)},\Z') -   \mu_0(Z_{i(-j)},\Z') \right\vert$. , For
a realization $\Z = \z$, the hypothesized randomization distribution of
$\mu_1(z_{i(-j)},\Z') - \mu_0(z_{i(-j)},\Z')$ is asymptotically normal with
mean $0$ and variance $V_{x(\z)} = 4 x(\z) \left[1-x(\z)\right] / N$, where
$x(\z) \equiv \sum_{ij\in U}z _{ij} /N$, if $0 < x(\z) <  1$.  (by the
Lindeberg CLT, Taylor linearization and Slutsky's theorem).  Thus the
$p$-value associated with randomization $\z$, $p = 2
\Phi(-|\mu_1(z_{i(-j)},\z) - \mu_0(z_{i(-j)},\z)|/V_{x(\z)}^{1/2})$. It
follows that $|\mu_1(z_{i(-j)},\z) - \mu_0(z_{i(-j)},\z)\vert <
-\Phi^{-1}(\alpha/2) V_{x(\z)}^{1/2} \Leftrightarrow p > \alpha$.  By similar
calculations at the household level, the (actual, not hypothesized)
randomization distribution of $\mu_1(Z_{i(-j)},\Z) - \mu_0(Z_{i(-j)},\Z)$ is
asymptotically mean $0$ with variance $2 V_{1/2} =  2/N$.\footnote{This result
  follows from reframing the problem as randomly assigning households into 4
  conditions \{00,01,10,11\} with equal probability, then linearizing
  $\mu_1(Z_{i(-j)},\Z) - \mu_0(Z_{i(-j)},\Z)$.} By the LLN ,  $\plim_{k
\rightarrow \infty} x = 1/2$, and so, by the and continuous mapping theorem,
$\lim_{k \rightarrow \infty} \Pr(\vert NV_{x(\Z)} - NV_{1/2}\vert >
\epsilon_3) = 0$.  Then, by Chebyshev's inequality, $\lim_{k \rightarrow
\infty}  \Pr(p > \alpha) > 1- 2\left[\Phi^{-1}(\alpha/2)\right]^{-2}$,
$\epsilon = 1- 2\left[\Phi^{-1}(\alpha/2)\right]^{-2}$ proportion of possible
$\Z \in \Omega$ have $p > \alpha$,
for any $0 < \alpha < 2\Phi(-\sqrt{2})$.  Tests based on effect increasing
statistics are therefore not generally consistent.

\subsection*{Alternative test statistics}

There are usually better ways to evaluate the fit of a model than comparing
two marginal distributions. One simple test statistic would be the
sum-of-squared-residuals (SSR) from a least squares regression of $\widetilde
y_{ij}(\0,\Z)$ on $Z_{ij}$ and $Z_{i(-j)}$. Since the model in \eqref{causal}
is linear, $\T(\widetilde y_{ij}(\0,\Z), \Z) = \sum_{ij \in U} \left[
  \widetilde \Y(\0) - (\X^T\X)^{-1}\X^T \widetilde \Y(\0) \right]^2$, where $$
\widetilde \Y(\0) \equiv \left( \begin{array}{c} \widetilde y_{11}(0) \\
    \vdots \\ y_{(N/2)(-1)}(0) \end{array} \right), \X \equiv  \left(
  \begin{array}{ccc} 1 & Z_{11} & Z_{1(-1)} \\ \vdots & \vdots & \vdots  \\ 1
    & Z_{(N/2)(-1)} & Z_{(N/2)1} \end{array} \right) = ({\bf 1} \; \Z \;
\Z^I).  $$ Assume now that either the hypothesized $\tau_1$ or $\tau_2$ is
radically incorrect: i.e., $\vert \widetilde\tau_1 - \tau_1 \vert$ or $\vert
\widetilde\tau_2 - \tau_2 \vert \gg \max(y_{ij}(\0)) - \min(y_{ij}(\0))$.
Fixing $\Z = \z$, for any randomization $\Z' \notin \{\z, (1-\z), \z^I,
(1-\z^I), \0, \one \}$, $\T(\widetilde y_{ij}(\0,\z),\Z') > \T(\widetilde
y_{ij}(\0,\z),\z)$. Assuming noncollinearity,
%$\Z \notin \{\Z^I, (1-\Z^I)\}$, , (0,...,0), (1,...,1)\}$)
$p \leq 6  \left( 2^{-N} \right)$, rejecting the hypothesis with $\alpha =
0.05$ for any $N \geq 8$. %Consistency follows straightforwardly.

In the example above, use of SSR as a test statistic has maximal power to
reject radically incorrect hypotheses, whereas the test statistics proposed by
BFP are inconsistent. But SSR from an ordinary least squares regression is not
always appropriate: for example, when the probability of exposure to spillover
is heterogeneous across individuals, we may wish to apply inverse probability
weights so as to ensure representative samples of potential outcomes. This
suggests a conjecture: that the $SSR$ from an {\it
  inverse-probability-weighted} least squares regression is more generally a
sensible test statistic for models that include interference.  (The nature of
these weights will be discussed when discussing alternative modes of
inference.) Additionally, when nonlinear deviations from model predictions are
of concern, a weighted variant of the Brownian distance covariance
\cite{szekely2009brownian} or other {\it E}-statistic may be more sensible
than SSR.

As an example of the performance of these new statistics, we re-analyze the
model and design from BFP. Recall that their model of treatment propagation
was:

\begin{equation}
\HH(\by_\bz, \bw, \beta, \tau) =
 \frac{\beta + (1 - w_i) (1 - \beta) \exp(- \tau^2 \bw^{T} \bS)}
      {\beta + (1 - z_i) (1 - \beta) \exp(- \tau^2 \bz^{T} \bS)} \by_\bz
\label{eq:spillovermodelA}
\end{equation}

Briefly, the model posits that treatment effects can depend on either direct
assignment to treatment ($\bz$) governed by $\beta$ or spillover as an
increasing (but flattening) function of the number of directly connected
treated neighbors ($\bz^{T} \bS$) and is governed by $\tau$. So, we have a
model with two parameters. The network used by BFP involves 256 nodes
connected in an undirected, random graph with node degree ranging from 0 to 10
(mean degree 4, 95\% of nodes with degree between 1 and 8, five nodes with
degree 0 [i.e. unconnected]).  Treatment is assigned to 50\% of the nodes at
completely at random in the BFP example.

The top row of figure~\ref{fig:twoD} compares the power of the SSR+Design test
statistic (upper left panel) to versions of this statistic that either only
include fixed node degree (SSR+Degree) , or no information about the network
at all (SSR). For each test statistic, we tested the hypothesis
$\tau=\tau_0,\beta=\tau_0$ by using a simulated permutation test (i.e. we used
1000 permutations instead of all of them). We executed that test 10,000 times
for each pair of parameters.  The proportion of $p$-values from that test less
than .05 is plotted in Figure~\ref{fig:twoD}: darker values are fewer
rejections, lighter values record more rejections.  All of these test
statistics are valid --- they reject the true null of $\tau=.5,\beta=2$ no
more than 5\% of the time at $\alpha=.05$ --- the plots are darkest in the
area where the two lines showing the true parameters intersect. All of the
plots have some power to reject non-true alternatives --- as we can see with
the large white areas in all of the plots. However, only when we add
information about the number of treated neighbors to the SSR+Degree
statistic, do we see high power against all alternatives in the plane.


\begin{figure}[h!]
\centering
\includegraphics[width=.99\textwidth]{twoDplots.pdf}
\caption{Proportion of $p$-values less than .05 for tests of joint hypotheses
about $\tau$ and $\beta$. Darker values mean rare rejection. White means
rejection always. Truth is shown at the intersection of the straight
lines  $\tau=.5, \beta=2$. Each panel shows a different test statistic. The
SSR Tests refer to eq, the KS tests refer to eq.  }\label{fig:twoD}
\end{figure}

The bottom row of Figure~\ref{fig:twoD} demonstrates the power of the KS test.
The bottom right hand panel shows the test used in BFP. Again all of the tests
are valid in the sense of rejecting the truth no more than 5\% of the time
when $\alpha=.05$ although all of these tests are conservative: the SSR based
tests rejected the truth roughly 4\% of the 10,000 simulations but the KS
tests rejected the truth roughly 2\% of the time.  The KS+Design and
KS+Degree panels show the power of applying the KS test to residuals from linear
models including network degree only (the +Degree version) or degree and also
the number of treated neighbors (a quantity that changes for each simulation
of the experiment). That is, where as the SSR panels used the sum of squared
residuals after accounting for network degree and/or number of treated
neighbors, the KS+Design and KS+Degree panels apply the KS test to the raw
residuals. These panels show (1) that inclusion of a quantity from the true
model (number of treated neighbors) is not enough to increase power against
all alternatives to the level shown by the SSR+Design test statistic and (2)
that the KS tests and the SSR tests have different patterns of power --- the
KS tests might be less powerful in general (more darker areas on the plots),
but our general sense from the analysis accords with the general sense about
the KS test in the scholarly literature --- that it is relatively low powered
(cites).

We cannot say here whether the SSR+Design test will provide the best power
against relevant alternatives for all possible models of treatment effect
propagation and designs. However, we hope that this research note both
improves the application of the BFP approach and raises new questions for
research.  Via counterexample, we can see that effect
increasing test statistics are not always appropriate for assessing hypotheses
about interference. BFP is correct in the assertion that, regardless of the
choice of test statistic selection, a set of implausible hypotheses is
identified by the procedure. But we should not be led to believe that, for any
given test statistic, that some hypotheses are more plausible than others.
Such inferences -- comparing hypotheses -- may depend on the test statistic
used, and not necessarily reflect the plausibility of the model at hand.


