

\section{Overview: Randomization based statistical inference for causal effects from social network experiments}

In a randomized experiment with $n=4$ subjects connected via a fixed network,
the response of subject $i=1$ might depend on the different ways that
treatment is assigned to the \emph{whole} network. When the treatment
assignment vector,$\bz$, provides treatment to persons 2 and 3,
$\bz=\{0,1,1,0\}$, person $i=1$ might respond one way,
$y_{i=1,\bz=\{0,1,1,0\}}$ and when treatment is assigned to persons 3 and 4,
$\bz=\{0,0,1,1\}$ person $i=1$ might act another way,
$y_{i=1,\bz=\{0,0,1,1\}}$. More generally, we might say that if the experiment had a causal effect on
person $i$, then her outcome would differ under different realizations of the
experimental treatment as a whole $y_{i,\bz} \ne y_{i,\bz'}$. The fundamental problem of
causal inference reminds us that we can never see both states of the world: we
only observe the outcome from person $i$ under one treatment assignment
vector, either $\bz$ or some $\bz'$ not both
\citep{holland:1986a,brady2008cae}.\footnote{In simpler settings, where
  treatment given to one individual has no effect on any other individual, we tend to
  write $y_{i,Z_i=1} \ne y_{i,Z_i=0}$ to say that treatment had a causal
  effect on person $i$.}

R.A. Fisher's approach to design-based statistical inference  \citep[Chap
2]{fisher:1935} as developed by Paul Rosenbaum  \citep{rosenbaum2010design}
begins with the premise of the fundamental problem of causal inference. Since
we cannot observe all of the ways that a given person would respond to
different treatments, the Fisher and Rosenbaum approach
suggests that we focus on learning about how \emph{models} of unobservable
counterfactual outcomes relate to what we can observe. Although we do not know
how person $i$ would have acted under all possible experimental scenarios, we
can learn how much information we have to dispel certain claims or hypotheses.
This conceptual move --- sidestepping the fundamental problem of causal
inference via learning about claims made by scientists --- drives hypothesis
testing in general.  BFP build on this insight by showing that models of
counterfactual effects can involve statements about how treatment given to one
node in a social network can influence other nodes. For example, they present
a model that allows the effects of treatment to die off as the network
distance between nodes increases.\footnote{We present this model later in this
  paper in equation~\ref{eq:spillovermodelA}. See the original paper itself for more
  details of the example model.} They also show that the strength of evidence
against the specific hypotheses implied by a given model varies with different
features of the research design as well as the extent to which the true causal
process diverged from the model. Since their simulated experiment involved two
treatments, the only observations available to evaluate the model were
comparisons of the assigned-to-treatment group and the assigned-to-control
group. Since their model could imply not only shifts in the mean of the
observed treatment versus control outcome distributions, but changes in shape
of those distributions, they used the Kolmogorov-Smirnov (KS) test statistic
so that their tests would be sensitive to differences in the treatment and
control distributions implied by different hypotheses and not only sensitive
to differences in one aspect of those distributions (such as the differences
in the mean).\footnote{If the empirical cumulative distribution function
  (ECDF) of the treated units is $F_1$ and the ECDF of the control units is
  $F_0$ then the KS test statistic is $\T(\yu,\bz)_{\text{KS}} = \underset{i =
    1,\ldots,n}{\text{max}}\left[F_1(y_{i,\bzero}) -
    F_0(y_{i,\bzero})\right]$, where $F(x)=(1/n)\sum_{i=1}^n I(x_i \le x)$
  records the proportion of the distribution of $x$ at or below $x_i$
  \citep[\S 5.4]{MylesHollander1999a}.\label{fn:kstest}} So, in broad
outline, the BFP approach involves (1) the articulation of a model for how a
treatment assignment vector can change outcomes for all subjects in the
experiment (holding the network fixed) and (2) the use of a function to
compare actually treated and control observations to summarize whether such a model is
implausible (codified as a low $p$-value)  or to report that we have too little
information available from the data and design about the model (codified as a
high $p$-value). This is classic hypothesis testing applied to an experiment
on a social network with sharp null hypotheses.

Say $Y_i$ is the observed outcome and we hypothesize that units do not
interfere and also that $y_{i,Z_i=1}=y_{i,Z_i=0}+\tau$. We can assess which
(if any) hypothesized values of $\tau$ appear implausible from the perspective
of the data by: (1) Mapping the hypothesis about unobserved quantities to
observed data using the identity $Y_i=Z_i y_{i,Z_i=1} + (1-Z_i) y_{i,Z_i=0}$
--- noticing that if $y_{i,Z_i=1}=y_{i,Z_i=0}+\tau$ then $y_{i,Z_i=0}=Y_i -
Z_i \tau$ (by substituting from the hypothesized relationship into the
observed data identity); (2) Using this result to adjust the observed outcome
to represent what would be implied by the hypothesis for a given $\tau_0$ such
that $\widetilde y_{i,Z_i=0}=Y_i - Z_i \tau_0$; and (3) Under the hypothesis,
$\widetilde y_{i,Z_i=0}$ should have no systematic relationship with treatment
assignment, so we can summarize this relationship with a test statistic,
$\mathcal T(\widetilde y_{i,Z_i=0},Z_i)$. A distribution of values for this
test statistic arises from repetitions of the treatment assignment process
(new draws of $\bz$ from all of the ways that such treatment assignment
vectors could have been produced); and finally (4) a $p$-value arises by
comparing the observed test statistic, $\mathcal T(Y_i,Z_i)$ against the
distribution of that test statistic that characterizes the hypothesis.

Notice that the test statistic choice matters in this process: the engine of
statistical inference involves summarizing information against the
hypothesized claim, yet different ways to summarize information might be more
or less sensitive to substantively meaningful differences. The statistical
power of a simple test of the sharp null hypothesis of no effects will vary as
a function of the design of the study (proportion treated, blocking structure,
etc), characteristics of the outcome (continuous, binary, skewed, extreme
points, etc), and the way that a test statistic summarizes the outcome (does
it compare means, standard deviations, medians, qqplots, etc). In general,
test statistics should be powerful against relevant alternatives. \citet[\S 2.4.4]{rosenbaum:2002} provides more specific
advice about the large sample performance of certain classes of test statistics
and BFP repeat his general advice: ``Select a test statistic [$\mathcal{T}$] that will be
small when the treated and control distributions in the adjusted data \ldots
are similar, and large when the distributions diverge.'' \cite[Proposition 4
and 5, \S 2.9]{rosenbaum:2002} presents results proving that test statistics
with this property (``effect increasing'' test statistics), produce an
unbiased test of the hypothesis of no effects or positive effects when the
positive effects involve one parameter.  Yet, when the models are complex and
may involve increasing effects in the direction of one parameter and
non-linear effects in the direction of another parameter, BFP showed that
sometimes a KS-test will have (1) no power to address the model at all such
that all hypothesized parameters would receive the same implausibility
assessment; (2) or might reject all such hypothesized parameters. Thus,
although in theory one may assess sharp multiparameter hypotheses,
 in practice one may not learn much from such tests. BFP thus recommended simulation studies of the operating
characteristics of tests as a piece of their workflow --- because  the theory
justifying simple one-dimensional effect increasing test statistics clearly
did not cover multi-parameter situations like those easily arising from social
network experiments.

\section{Hypothesis testing as model fit assessment: The SSR test statistic}

Rosenbaum style randomization inference tends to use test statistics that
compare two distributions. Simple models imply that the distribution of the
outcome in the control remains fixed. For example, $\widetilde
y_{i,Z_i=0}=Y_i-Z_i \tau$ only changes the distribution of outcomes for units
in the treated condition. Comparing the mean of $\widetilde y_{i,Z_i=1}|Z_i=1$
to the mean of $\widetilde y_{i,Z_i=0}|Z_i=0$ makes intuitive sense in this
case, and, if $Y_i$ is Normal or at least unimodal without major outliers,
then this test might have optimal power. The complex model used as an example
by BFP involved adjustments to both control and treated outcomes --- some
hypothesized parameters would cause shifts in variance, others in location.
So, BFP proposed to use the KS-test statistic to assess the relationship
between $\widetilde y_{i,Z_i=0,Z_{-i}=0}$ and $Z_i$ (where $Z_{-i}=0$ means
``when all units other than $i$ are also not treated".)

Yet, one can also think about the process of hypothesis testing as a process
of assessing model fit, and there are usually better ways to evaluate the fit
of a model than comparing two marginal distributions. In the case where we know
the fixed adjacency matrix of the network, $\bS$, and where we imagine that
network attributes (like degree) of a node play a role in the mechanism by
which treatment propagates, the idea of assessing model fit rather than
closeness of distributions leads naturally to the the sum-of-squared-residuals
(SSR) from a least squares regression of $ \widetilde y_{i,Z_i=0}$ on $Z_{i}$ and
$\bz^{T} \bS$ (i.e.\ the number of directly connected nodes assigned treatment)
as well as the $\mathbf{1}^{T} \bS$ (i.e.\ the degree of the node). If we
collected $Z_{i}$,$\bz^{T} \bS$, and $\mathbf{1}^{T} \bS$ into a matrix $\bX$,
and fit the $\widetilde y_{i,Z_i=0}$ as a linear function of $\bX$ with
coefficients $\bbeta$ when we could define the test statistic as:


\begin{equation}
 \T(\yu,\bz)_{\text{SSR}} \equiv \sum_{i} ( \widetilde y_{i,Z_i=0} -
 \bX\hat{\bbeta}) ^2 \label{eq:ssr}
\end{equation}

As an example of the performance of these new statistics, we re-analyze the
model and design from BFP. Their model of treatment propagation
was:

\begin{equation}
\HH(\by_\bz, \bw, \beta, \tau) =
 \frac{\beta + (1 - w_i) (1 - \beta) \exp(- \tau^2 \bw^{T} \bS)}
      {\beta + (1 - z_i) (1 - \beta) \exp(- \tau^2 \bz^{T} \bS)} \by_\bz
\label{eq:spillovermodelA}
\end{equation}

Briefly, this model posits that treatment effects can depend on either direct
assignment to treatment ($\bz$) governed by $\beta$ or spillover as an
increasing (but flattening) function of the number of directly connected
treated neighbors ($\bz^{T} \bS$) and is governed by $\tau$. So, we have a
model with two parameters. The network used by BFP involves 256 nodes
connected in an undirected, random graph with node degree ranging from 0 to 10
(mean degree 4, 95\% of nodes with degree between 1 and 8, five nodes with
degree 0 [i.e.\ unconnected]).  Treatment is assigned to 50\% of the nodes 
completely at random in the BFP example.

We assess three versions of the SSR test statistic versus three versions of
the KS test statistic. The first, described above, we call the SSR+Design test
statistic because it represents information about how treatment is assigned to
the nodes, $\bz^T \bS$. The second version of
the SSR test statistic (SSR+Degree) only includes network degree,  $\bOne^T \bS$, and
excludes information about the treatment status of other nodes. And the
third version (SSR) includes only treatment assignment $\bz$. The top row of
figure~\ref{fig:twoD} compares the power of the SSR+Design test statistic
(upper left panel) to versions of this statistic that either only include
fixed node degree (SSR+Degree), or no information about the network at all
(SSR). For each test statistic, we tested the hypothesis
$\tau=\tau_0,\beta=\tau_0$ by using a simulated permutation test: we
sampled 1000 permutations. We executed that test
10,000 times for each pair of parameters.  The proportion of $p$-values from
that test less than .05 is plotted in Figure~\ref{fig:twoD}: darker values
show fewer rejections, lighter values record more rejections.  All of these
test statistics are valid --- they reject the true null of $\tau=.5,\beta=2$
no more than 5\% of the time at $\alpha=.05$ --- the plots are darkest in the
area where the two lines showing the true parameters intersect. All of the
plots have some power to reject non-true alternatives --- as we can see with
the large white areas in all of the plots. Only when we add
information about the number of treated neighbors to the SSR+Degree statistic,
does the plot show high power against all alternatives in the plane.


\begin{figure}[h!] \centering
  \includegraphics[width=.99\textwidth]{twoDplots.pdf} \caption{Proportion of
    $p$-values less than .05 for tests of joint hypotheses about $\tau$ and
    $\beta$ for the model in equation~\ref{eq:spillovermodelA}. Darker values
    mean rare rejection. White means rejection always. Truth is shown at the
    intersection of the straight lines  $\tau=.5, \beta=2$. Each panel shows a
    different test statistic. The SSR Tests refer to equation~\ref{eq:ssr},
    the KS tests refer to the expression in
    footnote~\ref{fn:kstest}.}\label{fig:twoD}
\end{figure}

The bottom row of Figure~\ref{fig:twoD} demonstrates the power of the KS test.
The bottom right hand panel shows the test used in the BFP paper. Again all of the tests
are valid in the sense of rejecting the truth no more than 5\% of the time
when $\alpha=.05$ although all of these tests are conservative: the SSR based
tests rejected the truth roughly 4\% of the 10,000 simulations but the KS
tests rejected the truth roughly 2\% of the time.  The KS+Design and KS+Degree
panels show the power of applying the KS test to residuals from linear models
including network degree only (the +Degree version) or degree and also the
number of treated neighbors (the +Design version). That is, whereas the SSR
panels used the sum of squared residuals after accounting for network degree
and/or number of treated neighbors, the KS+Design and KS+Degree panels apply
the KS test to the raw residuals after adjusting for information about the
design and network (or with no adjustment). These panels show (1) that
inclusion of a quantity from the true model (number of treated neighbors) is
not enough to increase power against all alternatives to the level shown by
the SSR+Design test statistic and (2) that the KS tests and the SSR tests have
different patterns of power --- the KS tests appear be less powerful in general
(more darker areas on the plots).

\section{Application: Legislative Information Spillovers}

\citet{coppock2014information} presents an reanalysis of an experiment performed
by \citet{butlernickerson2011}. Leading up to a key vote on a budget bill, SB24, in the
New Mexico state senate, legislators were randomly assigned to be given
constituent survey results, and their votes on SB24 recorded. The original
analysis found that legislators assigned to receive constituent information were
more likely to vote consistently with the preferences of their district.
Given the easy nature of sharing information in
field experiments \citep{Winters:2012fk}, Coppock reanalyzes the
experimental results with an eye towards spillover, constructing a social
network based on the similarity of W-NOMINATE ideology scores. Coppock hypothesized
that ideologically similar legislators would be more likely to share
information. If the constituent survey information does induce a change in behavior, this
effect may be observed not only in those legislators that are directly treated
in the experiment, but also by those of similar ideology to those that are
treated.

With distance matrix $\Gamma$, Coppock evaluated the model:

$$\yu = \yz - \beta_1 \bz - \beta_2 g(\Gamma \bz)$$

Where $g$ is a function that normalizes the sums of distances to treated
neighbors to have unit variance. This model has a direct
effect ($\beta_1$) and an indirect effect ($\beta_2$) that is linear in the distances to
treated neighbors. Coppock evaluated this spillover model using the SSR+Design statistic as
presented in this paper. To demonstrate the value of this statistic,
particularly for models that are linear in the spillover effects, we repeat
Coppock's analysis using both the SSR+Design statistic and a KS statistic. The
results in Figure~\ref{fig:coppock-replication} recreate Coppock's Figure
2.\footnote{During the replication process, we noticed an inconsistency between
  our results using the SSR+Design statistic and Coppock's published figure. In
  coordination with the author, we discovered that the published figure suffered
  from on a programming error. The figure in this document reflects a correction
  to that error, so it will not precisely match Coppock's original publication.}
The KS test statistic, while being more discerning in the neighborhood of (0,
0), fails to bound the region of plausible hypotheses in the manner of the
SSR+Design statistic. We find this quality of the test statistic to be
particularly valuable. A powerful test statistic is a useful
tool, but equally useful is the ability to succinctly describe the region in
which we would fail to reject a null hypothesis. In this case, the SSR statistic
provides such a region, making discussing and evaluating the result's
of Coppock's analysis much more straightforward: it would be very unlikely that
the data were generated from the model with parameters outside the rectangle
with corners (-0.6, 0.2) and (0.3, -0.4). 

\begin{figure}[h!] \centering
  \includegraphics[width=.99\textwidth]{../coppock-replication/CoppockJEPS_figure2.pdf}
  \caption{Replication of Figure 2 from \citet{coppock2014information}: plotting
    $p$-values for direct and indirect model parameters. The
    original analysis used the SSR statistic, which nicely bounds the region of
    plausible hypotheses. The KS statistic, while eliminating more hypotheses in
  the neighborhood of (0, 0), fails to provide any such bounds.}
  \label{fig:coppock-replication}
\end{figure}

\section{Discussion and Speculations}

We cannot say here whether the SSR+Design test will provide the best power
against relevant alternatives for all possible models of treatment effect
propagation, network topologies and designs. However, we hope that this
research note both improves the application of the BFP approach and raises new
questions for research.  BFP are correct in the assertion that, regardless of
the choice of test statistic selection, a set of implausible hypotheses is
identified by the procedure. But we should not be led to believe that, for any
given test statistic, that some hypotheses are universally more plausible than others.
Such inferences --- comparing hypotheses --- may depend on the test statistic
used, and not necessarily reflect the plausibility of the model at hand. That
is, the results of any hypothesis test (or confidence interval creation) tell
us \emph{both} about the test statistic \emph{and} about the causal model under scrutiny.

In the example above, the SSR+Design test statistic had much better power than
any other test statistic. But SSR from an ordinary least squares regression is
not always appropriate: for example, when the probability of exposure to
spillover is heterogeneous across individuals in a way not well captured by the
$\bz^T \bS$ term or some other analogous term, we may wish to apply inverse
probability weights so as to ensure representative samples of potential
outcomes. This suggests a conjecture: that the $SSR$ from an {\it
  inverse-probability-weighted} least squares regression is more generally a
sensible test statistic for models that include
interference.\footnote{\citet{aronowsamii2012interfere} use such weights for
  unbiased estimation of network-treatment-exposure probability weighted
  average treatment effects.}  Additionally, when nonlinear deviations from
model predictions are of concern, a weighted variant of the Brownian distance
covariance \cite{szekely2009brownian} or other {\it E}-statistic may be more
sensible than SSR.


