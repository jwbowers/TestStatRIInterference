
 \citet{bowers2013sutva} showed that one could use Fisher's
 randomization-based hypothesis testing framework to assess models of
 treatment propagation and spillover across social networks. Working in a
 frequentist framework, they showed that one could make statements such as:
 "Our design provides strong evidence against the idea that the treatment
 spilled over by more than 5 points given this model of treatment
 propagation." This research note shows that the test statistic that they used
 --- the Kolmogorov-Smirnov test statistic  \citep[\S
 5.4]{MylesHollander1999a} --- could be improved substantially. We replicate
 their analyses to demonstrate that the sum of squared residuals from a linear
 model in which aspects of the network itself play a role. 

 

 They also showed that the strength of evidence against the
 specific hypotheses implied by a given model varied with different features
 of the design as well as the extent to which the true process by which
 treatment given to one unit changed the outcomes of another unit diverged
 from the model. Throughout their paper they used the  Kolmogorov-Smirnov (KS)
 test statistic so that their tests would be sensitive differences in the
 treatment and control distributions implied by different hypotheses and not
 merely sensitive to differences in one aspect of those distributions (such as
 the differences in the mean).\footnote{If the empirical cumulative distribution function (ECDF) of
   the treated units is $F_1$ and the ECDF of the control units is $F_0$ then
   the KS test statistic is
   $\T(\yu,\bz) = \underset{i = 1,\ldots,n}{\text{max}}\left[F_1(y_{i,\bzero})
     - F_0(y_{i,\bzero})\right]$, where $F(x)=(1/n)\sum_{i=1}^n I(x_i \le x)$
   records the proportion of the distribution of $x$ at or below $x_i$
   \citep[\S 5.4]{MylesHollander1999a}.} 
 
 provides both clarity for
 non-parametric statistical inference  also implied great flexibility in regard to the scientific questions
that one might engage: for example, one could use the sharp null itself to
detect spillover  among units \citep{aronow-general} or to characterize the
benefits from treatment on treated units even if  and sharp hypotheses
about effects (in contrast to hypotheses about no effects) have been developed
to assess models of interference \citep{bowers2013sutva}, [ include Rosenbaum
2007, Bowers and Hansen 2009, and a bunch of other stuff by Rosenbaum ].


Fisher's sharp null hypothesis of no effects combined with his
randomization-based hypothesis testing framework provides a very clear and
clean vision of what it means to test a hypothesis \citep[Chap
2]{fisher:1935} and \citep{rosenbaum2010design}.
One problem that has become apparent, as scholars have extended Fisher's basic
ideas, is that as the models of effects have become more complex, the power of
the testing procedure itself has become more important. A simple test of the
sharp null can be executed with power that varies as a function of the design
of the study (proportion treated, blocking structure, etc..), characteristics
of the outcome, and the way that a test statistic summarizes the outcome. Yet,
when the models are complex,  \citet{bowers2013sutva} showed that sometimes a
test will have (1) no power to address the model at all (a function, in part
of the model itself --- for example ...) such that all hypothesized parameters
would receive the same implausibility assessment; (2) or might reject all such
hypothesized parameters. Thus, although in theory one may assess sharp
hypotheses about multiparameter settings, in practice one may not learn much
from such tests --- or, in the absence of simulation studies assessing the
model and test statistic, one may be mislead.  

In this paper we use a very simple causal model and a simple test statistic to
first show how the combination of model and test statistic is a crucial choice
--- and how more attention ought to be paid to both the model and the test
statistic.

Then we suggest one test statistic that we think should have good properties
when the number of parameters in a model is low (i.e. $k << n$). And we should
this to be the case --- that the robust F-stat from a linear model is a better
way to go than the KS-test and certainly better than any single measure of
distributional difference.

Finally, we highlight the similarities between testing the sharp hypothesis
and the weak hypothesis --- which in turn allows even more flexibility and
perhaps better performing testing procedures.


Then we suggest a few alternative avenues to pursue.

Some models are well powered for some designs and not other designs.

Using same simulation study using same data from PA study. Better than KS
test. 

The F-test is model consistent. By design it controls Type I error, but if the
model is correct, it has some optimality criterion. If you had control
variables, for example, then it would be consistent against mean differences.

Under the model, model consistent.


