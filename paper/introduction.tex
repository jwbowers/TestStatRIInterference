
\citet{bowers2013sutva} showed that one could use Fisher's randomization-based
hypothesis testing framework to assess counterfactual causal models of
treatment propagation and spillover across social networks.  This research
note improves the statistical infernce presented in \citet{bowers2013sutva}
(henceforth BFP) by substituting a test statistic based on a sum of squared
residuals and incorporating information about the fixed network for the simple
the Kolmogorov-Smirnov test statistic  \citep[\S 5.4]{MylesHollander1999a}
that they used. The point of this short article is to incrementally improve
the use of BFP's ``reasoning about interference'' approach. We do not offer
results about test statistics for multi-parameter causal models on social
networks here, but instead hope that this research note stimulates further,
and deeper, work on test statistics and sharp hypotheses.

\section{Background on randomization based statistical inference for questions
about causal effects from social network experiments}

In a randomized experiment with $n=4$ subjects connected via a fixed network,
subject $i=1$ might respond differently to different ways that treatment is
assigned to the whole network. When the treatment assignment vector provides
treatment to persons 2 and 3, $\bz=\{0,1,1,0\}$ she might act one way,
$y_{i=1,\bz=\{0,1,1,0\}}$ and when treatment is assigned to persons 3 and 4,
$\bz=\{0,0,1,1\}$ she might act another way, $y_{i=1,\bz=\{0,0,1,1\}}$ That
is, if the experiment had a causal effect on person $i$, then her outcome
would differ under different realizations of the experimental treatment
$y_{i,\bz} \ne y_{i,\bz'}$. The fundamental problem of causal inference
reminds us that we can never see both states of the world: we only observe the
outcome from person $i$ under one treatment assignment vector, either $\bz$ or
some $\bz'$ not both  \cite{holland:1986a}.

R.A. Fisher's approach to design-based statistical inference  \citep[Chap
2]{fisher:1935} as developed by Paul Rosenbaum  \citep{rosenbaum2010design}
begins with the premise of the fundamental problem of causal inference. Rather
than imagine that we could learn directly about the unobservable
counterfactual comparisons of outcomes under different treatment vectors,
Fisher would suggest that we focus on learning about how \emph{models} of
those counterfactual outcomes relate to what we observe. Although we do not
know how person $i$ would have acted under all possible experimental
scenarios, we can learn how much information we have to dispel certain claims
or hypotheses.  This conceptual move --- sidestepping the fundamental problem
of causal inference via learning about claims made by scientists --- drives
hypothesis testing in general.  BFP build on this insight by showing that
models of counterfactual effects can involve statements about how treatment
given to one node in a social network can come to have an effect on other
nodes. Their example model allows the effects of treatment to die off as the
network distance between nodes increases.\footnote{See the paper itself for
  details of the example model.} They further showed that the strength of
evidence against the specific hypotheses implied by a given model varied with
different features of the research design as well as the extent to which the
true causal process diverged from the model. Since their simulated experiment
involved only two treatments, the only observations available to evaluate the
model were comparisons of the assigned to treatment group and the assigned to
control group. Since their model could imply not only shifts in the mean of
the observed treatment versus control outcome distributions, but changes in
shape of those distributions, they used the Kolmogorov-Smirnov (KS) test
statistic so that their tests would be sensitive differences in the treatment
and control distributions implied by different hypotheses and not merely
sensitive to differences in one aspect of those distributions (such as the
differences in the mean).\footnote{If the empirical cumulative distribution
  function (ECDF) of the treated units is $F_1$ and the ECDF of the control
  units is $F_0$ then the KS test statistic is $\T(\yu,\bz) = \underset{i =
    1,\ldots,n}{\text{max}}\left[F_1(y_{i,\bzero}) -
    F_0(y_{i,\bzero})\right]$, where $F(x)=(1/n)\sum_{i=1}^n I(x_i \le x)$
  records the proportion of the distribution of $x$ at or below $x_i$
  \citep[\S 5.4]{MylesHollander1999a}.} So, in broad outline, the BFP approach
involves (1) the articulation of a model for how a treatment assignment vector
can change outcomes for all subjects in the experiment (holding the network
fixed) and (2) using function of comparing actually treated and control
observations to tell us whether such a model is implausible (codified as a low
$p$-value)  or whether we have too little information available from the data
and design about a the model (codified as a high $p$-value). This is classic
hypothesis testing.

So, say $Y_i$ is the observed outcome and we hypothesize that units do not
interfere with each other and also that $y_{i,Z_i=1}=y_{i,Z_i=0}+\tau$, we can
assess which (if any) hypothesized values of $\tau$ appear implausible from
the perspective of the data by: (1) Mapping the hypothesis about unobserved
quantities using the identity $Y_i=Z_i y_{i,Z_i=1} + (1-Z_i) y_{i,Z_i=0}$ ---
noticing that if $y_{i,Z_i=1}=y_{i,Z_i=0}+\tau$ then $y_{i,Z_i=0}=Y_i - Z_i
\tau$ (by substituting from the hypothesized relationship into the observed
data identity); (2) Using this result to adjust the observed outcome to
represent what would be implied by the hypothesis for given $\tau_0$ such that
$\widetilde y_{i,Z_i=0}=Y_i - Z_i \tau_0$; and (3) Under the hypothesis,
$\widetilde y_{i,Z_i=0}$ should have no systematic relationship with treatment
assignment, so we can summarize this relationship with a test statistic,
$\mathcal T(\widetilde y_{i,Z_i=0},Z_i)$ and the distribution of values for
this test statistic arising from repetitions of treatment assignment (new
draws of $\bz$ from all of the ways that such treatment assignment vectors
could have been produced); and finally (4) a $p$-value arises by comparing the
observed test statistic, $T(Y_i,Z_i)$ against the distribution of that test
statistic that characterizes the hypothesis.

Notice that the test statistic choice matters in this process: the engine of
statistical inference involves summarizing information against the
hypothesized claim, yet different ways to summarize information might be more
or less sensitive to substantively meaningful differences. The statistical
power of a simple test of the sharp null hypothesis of no effects will vary as
a function of the design of the study (proportion treated, blocking structure,
etc.), characteristics of the outcome (continuous, binary, skewed, extreme
points, etc.), and the way that a test statistic summarizes the outcome (does
it compare means, standard deviations, medians, qqplots, etc.). In general,
test statistics should be powerful against relevant alternatives (find
canonical cite). \citet[\S 2.4.4]{rosenbaum:2002} provides more specific
advice about large sample performance of certain classes of test statistics
and BFP repeat his general advice: `` Select a test statistic T that will be
small when the treated and control distributions in the adjusted data \ldots
are similar, and large when the distributions diverge.'' \citet[Proposition 4
and 5, \S 2.9]{rosenbaum:2002} presents results proving that test statistics
with this property (``effect increasing'' test statistics), produces an
unbiased test of the hypothesis of no effects or positive effects when the
positive effects involve one parameter.  Yet, when the models are complex and
may involve increasing effects in the direction of one parameter and
non-linear effects in the direction of another parameter, BFP showed that
sometimes a KS-test will have (1) no power to address the model at all such
that all hypothesized parameters would receive the same implausibility
assessment; (2) or might reject all such hypothesized parameters. Thus,
although in theory one may assess sharp hypotheses about multiparameter
settings, in practice one may not learn much from such tests --- or, in the
absence of simulation studies assessing the model and test statistic, one may
be misled. BFP thus recommended simulation studies of the operating
characteristics of tests as a piece of their workflow --- because  the theory
justifying simple one-dimensional effect increasing test statistics clearly
did not cover multi-parameter situations like those easily arising from social
network experiments.

\section{Hypothesis testing as model fit assessment: The SSR test statistic}

Rosenbaum style randomization inference tends to use test statistics that
compare two distributions. Simple models imply that the distribution of the
outcome in the control remains fixed. For example, $\widetilde
y_{i,Z_i=0}=Y_i-Z_i \tau$ only changes the distribution of outcomes for units
in the treated condition. Comparing the mean of $\widetilde y_{i,Z_i=0}|Z_i=0$
to the mean of $\widetilde y_{i,Z_i=0}|Z_i=0$ makes intuitive sense in this
case, and, if $Y_i$ is Normal or at least unimodal without major outliers,
then this test might have optimal power. The complex model used as an example
by BFP involved adjustments to both control and treated outcomes --- some
hypothesized parameters would cause shifts in variance, others in location.
So, BFP proposed to use the KS-test statistic to assess the relationship
between $\widetilde y_{i,Z_i=0,Z_{-i}=0}$ and $Z_i$ (where $Z_{-i}=0$ means
"when all units other than $i$ are also not treated".

Yet, one can also think about the process of hypothesis testing as a process
of assessing model fit, and there are usually better ways to evaluate the fit
of a model than comparing two marginal distributions. In the case were we know
the fixed adjacency matrix of the network, $\bS$, and where we imagine that
network attributes (like degree) of a node play a role in the mechanism by
which treatment propagates, the idea of assessing model fit rather than
distribution comparison leads naturally to the the sum-of-squared-residuals
(SSR) from a least squares regression of $ y_{i,Z_i=0}$ on $Z_{i}$ and
$\bz^{T} \bS$ (i.e. the number of directly connected nodes assigned treatment)
as well as the $\mathbf{1}^{T} \bS$ (i.e. the degree of the node). 

As an example of the performance of these new statistics, we re-analyze the
model and design from BFP. Their model of treatment propagation
was:

\begin{equation}
\HH(\by_\bz, \bw, \beta, \tau) =
 \frac{\beta + (1 - w_i) (1 - \beta) \exp(- \tau^2 \bw^{T} \bS)}
      {\beta + (1 - z_i) (1 - \beta) \exp(- \tau^2 \bz^{T} \bS)} \by_\bz
\label{eq:spillovermodelA}
\end{equation}

Briefly, this model posits that treatment effects can depend on either direct
assignment to treatment ($\bz$) governed by $\beta$ or spillover as an
increasing (but flattening) function of the number of directly connected
treated neighbors ($\bz^{T} \bS$) and is governed by $\tau$. So, we have a
model with two parameters. The network used by BFP involves 256 nodes
connected in an undirected, random graph with node degree ranging from 0 to 10
(mean degree 4, 95\% of nodes with degree between 1 and 8, five nodes with
degree 0 [i.e. unconnected]).  Treatment is assigned to 50\% of the nodes at
completely at random in the BFP example.

We assess three versions of the SSR test statistic versus three versions of
the KS test statistic. The first, describe above, we call the SSR+Design test
statistic because it represents information about how treatment is assigned to
the nodes (in $\bz^T \bS$). The second version of the SSR test statistic only
includes network degree and excludes information about the treatment status of
other nodes (only includes $\b1^T \bS$). And the third version includes only
treatment assignment $\bz$. The top row of figure~\ref{fig:twoD} compares the
power of the SSR+Design test statistic (upper left panel) to versions of this
statistic that either only include fixed node degree (SSR+Degree), or no
information about the network at all (SSR). For each test statistic, we tested
the hypothesis $\tau=\tau_0,\beta=\tau_0$ by using a simulated permutation
test (i.e. we sampled 1000 permutations instead of all of them). We executed that
test 10,000 times for each pair of parameters.  The proportion of $p$-values
from that test less than .05 is plotted in Figure~\ref{fig:twoD}: darker
values show fewer rejections, lighter values record more rejections.  All of
these test statistics are valid --- they reject the true null of
$\tau=.5,\beta=2$ no more than 5\% of the time at $\alpha=.05$ --- the plots
are darkest in the area where the two lines showing the true parameters
intersect. All of the plots have some power to reject non-true alternatives
--- as we can see with the large white areas in all of the plots. However,
only when we add information about the number of treated neighbors to the
SSR+Degree statistic, do we see high power against all alternatives in the
plane.


\begin{figure}[h!]
\centering
\includegraphics[width=.99\textwidth]{twoDplots.pdf}
\caption{Proportion of $p$-values less than .05 for tests of joint hypotheses
about $\tau$ and $\beta$. Darker values mean rare rejection. White means
rejection always. Truth is shown at the intersection of the straight
lines  $\tau=.5, \beta=2$. Each panel shows a different test statistic. The
SSR Tests refer to eq, the KS tests refer to eq.  }\label{fig:twoD}
\end{figure}

The bottom row of Figure~\ref{fig:twoD} demonstrates the power of the KS test.
The bottom right hand panel shows the test used in BFP. Again all of the tests
are valid in the sense of rejecting the truth no more than 5\% of the time
when $\alpha=.05$ although all of these tests are conservative: the SSR based
tests rejected the truth roughly 4\% of the 10,000 simulations but the KS
tests rejected the truth roughly 2\% of the time.  The KS+Design and KS+Degree
panels show the power of applying the KS test to residuals from linear models
including network degree only (the +Degree version) or degree and also the
number of treated neighbors (the +Design version). That is, whereas the SSR
panels used the sum of squared residuals after accounting for network degree
and/or number of treated neighbors, the KS+Design and KS+Degree panels apply
the KS test to the raw residuals after adjusting for information about the
design and network (or with no adjustment). These panels show (1) that
inclusion of a quantity from the true model (number of treated neighbors) is
not enough to increase power against all alternatives to the level shown by
the SSR+Design test statistic and (2) that the KS tests and the SSR tests have
different patterns of power --- the KS tests might be less powerful in general
(more darker areas on the plots), but our general sense from the analysis
accords with the general sense about the KS test in the scholarly literature
--- that it is relatively low powered.\footnote{For a short description and a
few citations to the well-known low-power of the KS-test, see
\url{https://asaip.psu.edu/Articles/beware-the-kolmogorov-smirnov-test}}

\section{Discussion and Speculations}

We cannot say here whether the SSR+Design test will provide the best power
against relevant alternatives for all possible models of treatment effect
propagation, network topologies and designs. However, we hope that this
research note both improves the application of the BFP approach and raises new
questions for research.  BFP is correct in the assertion that, regardless of
the choice of test statistic selection, a set of implausible hypotheses is
identified by the procedure. But we should not be led to believe that, for any
given test statistic, that some hypotheses are more plausible than others.
Such inferences -- comparing hypotheses -- may depend on the test statistic
used, and not necessarily reflect the plausibility of the model at hand. That
is, the results of any hypothesis test (or confidence interval creation) tell
us both about the test statistic and about the causal model under scrutiny.

In the example above, the SSR+Design test statistic had much better power than
any other test statistic. But SSR from an ordinary least squares regression is
not always appropriate: for example, when the probability of exposure to
spillover is heterogeneous across individuals and not well captured by the
$\bz^T \bS$ term or some other analogous term, we may wish to apply inverse
probability weights so as to ensure representative samples of potential
outcomes. This suggests a conjecture: that the $SSR$ from an {\it
  inverse-probability-weighted} least squares regression is more generally a
sensible test statistic for models that include
interference.\footnote{\citet{aronowsamii2012interfere} use such weights for
  unbiased estimation of network-treatment-exposure probability weighted
  average treatment effects.}  Additionally, when nonlinear deviations from
model predictions are of concern, a weighted variant of the Brownian distance
covariance \cite{szekely2009brownian} or other {\it E}-statistic may be more
sensible than SSR.


