
\citet{bowers2013sutva} showed that one could use Fisher's randomization-based
hypothesis testing framework to assess counterfactual causal models of
treatment propagation and spillover across social networks.  This research
note improves the statistical infernce presented in \citet{bowers2013sutva}
(henceforth BFP) by substituting a test statistic based on a sum of squared
residuals and incorporating information about the fixed network for the simple
the Kolmogorov-Smirnov test statistic  \citep[\S 5.4]{MylesHollander1999a}
that they used. The point of this short article is to incrementally improve
the use of BFP's ``reasoning about interference'' approach. We do not offer
results about test statistics for multi-parameter causal models on social
networks here, but instead hope that this research note stimulates further,
and deeper, work on test statistics and sharp hypotheses.

\section{Background on randomization based statistical inference for questions
about causal effects from social network experiments}

In a randomized experiment with $n=4$ subjects connected via a fixed network,
subject $i=1$ might respond differently to different ways that treatment is
assigned to the whole network. When the treatment assignment vector provides
treatment to persons 2 and 3, $\bz=\{0,1,1,0\}$ she might act one way,
$y_{i=1,\bz=\{0,1,1,0\}}$ and when treatment is assigned to persons 3 and 4,
$\bz=\{0,0,1,1\}$ she might act another way, $y_{i=1,\bz=\{0,0,1,1\}}$ That
is, if the experiment had a causal effect on person $i$, then her outcome
would differ under different realizations of the experimental treatment
$y_{i,\bz} \ne y_{i,\bz'}$. The fundamental problem of causal inference
reminds us that we can never see both states of the world: we only observe the
outcome from person $i$ under one treatment assignment vector, either $\bz$ or
some $\bz'$ not both  \cite{holland:1986a}.

R.A. Fisher's approach to design-based statistical inference  \citep[Chap
2]{fisher:1935} as developed by Paul Rosenbaum  \citep{rosenbaum2010design}
begins with the premise of the fundamental problem of causal inference. Rather
than imagine that we could learn directly about the unobservable
counterfactual comparisons of outcomes under different treatment vectors,
Fisher would suggest that we focus on learning about how \emph{models} of
those counterfactual outcomes relate to what we observe. Although we do not
know how person $i$ would have acted under all possible experimental
scenarios, we can learn how much information we have to dispel certain claims
or hypotheses.  This conceptual move --- sidestepping the fundamental problem
of causal inference via learning about claims made by scientists --- drives
hypothesis testing in general.  BFP build on this insight by showing that
models of counterfactual effects can involve statements about how treatment
given to one node in a social network can come to have an effect on other
nodes. Their example model allows the effects of treatment to die off as the
network distance between nodes increases.\footnote{See the paper itself for
  details of the example model.} They further showed that the strength of
evidence against the specific hypotheses implied by a given model varied with
different features of the research design as well as the extent to which the
true causal process diverged from the model. Since their simulated experiment
involved only two treatments, the only observations available to evaluate the
model were comparisons of the assigned to treatment group and the assigned to
control group. Since their model could imply not only shifts in the mean of
the observed treatment versus control outcome distributions, but changes in
shape of those distributions, they used the Kolmogorov-Smirnov (KS) test
statistic so that their tests would be sensitive differences in the treatment
and control distributions implied by different hypotheses and not merely
sensitive to differences in one aspect of those distributions (such as the
differences in the mean).\footnote{If the empirical cumulative distribution
  function (ECDF) of the treated units is $F_1$ and the ECDF of the control
  units is $F_0$ then the KS test statistic is $\T(\yu,\bz) = \underset{i =
    1,\ldots,n}{\text{max}}\left[F_1(y_{i,\bzero}) -
    F_0(y_{i,\bzero})\right]$, where $F(x)=(1/n)\sum_{i=1}^n I(x_i \le x)$
  records the proportion of the distribution of $x$ at or below $x_i$
  \citep[\S 5.4]{MylesHollander1999a}.} So, in broad outline, the BFP approach
involves (1) the articulation of a model for how a treatment assignment vector
can change outcomes for all subjects in the experiment (holding the network
fixed) and (2) using function of comparing actually treated and control
observations to tell us whether such a model is implausible (codified as a low
$p$-value)  or whether we have too little information available from the data
and design about a the model (codified as a high $p$-value). This is classic
hypothesis testing.

Notice that the test statistic choice matters in this process: the engine of
statistical inference involves summarizing information against the
hypothesized claim, yet different ways to summarize information might be more
or less sensitive to substantively meaningful differences. The statistical
power of a simple test of the sharp null hypothesis of no effects will vary as
a function of the design of the study (proportion treated, blocking structure,
etc.), characteristics of the outcome (continuous, binary, skewed, extreme
points, etc.), and the way that a test statistic summarizes the outcome (does
it compare means, standard deviations, medians, qqplots, etc.). In general,
test statistics should be powerful against relevant alternatives (find
canonical cite). \citet[\S 2.4.4]{rosenbaum:2002} provides more specific
advice about large sample performance of certain classes of test statistics
and BFP repeat his general advice: `` Select a test statistic T that will be
small when the treated and control distributions in the adjusted data \ldots
are similar, and large when the distributions diverge.'' \citet[Proposition 4
and 5, \S 2.9]{rosenbaum:2002} presents results proving that test statistics
with this property (``effect increasing'' test statistics), produces an
unbiased test of the hypothesis of no effects or positive effects when the
positive effects involve one parameter.  Yet, when the models are complex and
may involve increasing effects in the direction of one parameter and
non-linear effects in the direction of another parameter, BFP showed that
sometimes a KS-test will have (1) no power to address the model at all such
that all hypothesized parameters would receive the same implausibility
assessment; (2) or might reject all such hypothesized parameters. Thus,
although in theory one may assess sharp hypotheses about multiparameter
settings, in practice one may not learn much from such tests --- or, in the
absence of simulation studies assessing the model and test statistic, one may
be misled. BFP thus recommended simulation studies of the operating
characteristics of tests as a piece of their workflow --- because  the theory
justifying simple one-dimensional effect increasing test statistics clearly
did not cover multi-parameter situations like those easily arising from social
network experiments.

\subsection*{Alternative test statistics}

There are usually better ways to evaluate the fit of a model than comparing
two marginal distributions. One simple test statistic would be the
sum-of-squared-residuals (SSR) from a least squares regression of $\widetilde
y_{ij}(\0,\Z)$ on $Z_{ij}$ and $Z_{i(-j)}$. Since the model in \eqref{causal}
is linear, $\T(\widetilde y_{ij}(\0,\Z), \Z) = \sum_{ij \in U} \left[
  \widetilde \Y(\0) - (\X^T\X)^{-1}\X^T \widetilde \Y(\0) \right]^2$, where $$
\widetilde \Y(\0) \equiv \left( \begin{array}{c} \widetilde y_{11}(0) \\
    \vdots \\ y_{(N/2)(-1)}(0) \end{array} \right), \X \equiv  \left(
  \begin{array}{ccc} 1 & Z_{11} & Z_{1(-1)} \\ \vdots & \vdots & \vdots  \\ 1
    & Z_{(N/2)(-1)} & Z_{(N/2)1} \end{array} \right) = ({\bf 1} \; \Z \;
\Z^I).  $$ Assume now that either the hypothesized $\tau_1$ or $\tau_2$ is
radically incorrect: i.e., $\vert \widetilde\tau_1 - \tau_1 \vert$ or $\vert
\widetilde\tau_2 - \tau_2 \vert \gg \max(y_{ij}(\0)) - \min(y_{ij}(\0))$.
Fixing $\Z = \z$, for any randomization $\Z' \notin \{\z, (1-\z), \z^I,
(1-\z^I), \0, \one \}$, $\T(\widetilde y_{ij}(\0,\z),\Z') > \T(\widetilde
y_{ij}(\0,\z),\z)$. Assuming noncollinearity,
%$\Z \notin \{\Z^I, (1-\Z^I)\}$, , (0,...,0), (1,...,1)\}$)
$p \leq 6  \left( 2^{-N} \right)$, rejecting the hypothesis with $\alpha =
0.05$ for any $N \geq 8$. %Consistency follows straightforwardly.

In the example above, use of SSR as a test statistic has maximal power to
reject radically incorrect hypotheses, whereas the test statistics proposed by
BFP are inconsistent. But SSR from an ordinary least squares regression is not
always appropriate: for example, when the probability of exposure to spillover
is heterogeneous across individuals, we may wish to apply inverse probability
weights so as to ensure representative samples of potential outcomes. This
suggests a conjecture: that the $SSR$ from an {\it
  inverse-probability-weighted} least squares regression is more generally a
sensible test statistic for models that include interference.  (The nature of
these weights will be discussed when discussing alternative modes of
inference.) Additionally, when nonlinear deviations from model predictions are
of concern, a weighted variant of the Brownian distance covariance
\cite{szekely2009brownian} or other {\it E}-statistic may be more sensible
than SSR.

As an example of the performance of these new statistics, we re-analyze the
model and design from BFP. Recall that their model of treatment propagation
was:

\begin{equation}
\HH(\by_\bz, \bw, \beta, \tau) =
 \frac{\beta + (1 - w_i) (1 - \beta) \exp(- \tau^2 \bw^{T} \bS)}
      {\beta + (1 - z_i) (1 - \beta) \exp(- \tau^2 \bz^{T} \bS)} \by_\bz
\label{eq:spillovermodelA}
\end{equation}

Briefly, the model posits that treatment effects can depend on either direct
assignment to treatment ($\bz$) governed by $\beta$ or spillover as an
increasing (but flattening) function of the number of directly connected
treated neighbors ($\bz^{T} \bS$) and is governed by $\tau$. So, we have a
model with two parameters. The network used by BFP involves 256 nodes
connected in an undirected, random graph with node degree ranging from 0 to 10
(mean degree 4, 95\% of nodes with degree between 1 and 8, five nodes with
degree 0 [i.e. unconnected]).  Treatment is assigned to 50\% of the nodes at
completely at random in the BFP example.

The top row of figure~\ref{fig:twoD} compares the power of the SSR+Design test
statistic (upper left panel) to versions of this statistic that either only
include fixed node degree (SSR+Degree) , or no information about the network
at all (SSR). For each test statistic, we tested the hypothesis
$\tau=\tau_0,\beta=\tau_0$ by using a simulated permutation test (i.e. we used
1000 permutations instead of all of them). We executed that test 10,000 times
for each pair of parameters.  The proportion of $p$-values from that test less
than .05 is plotted in Figure~\ref{fig:twoD}: darker values are fewer
rejections, lighter values record more rejections.  All of these test
statistics are valid --- they reject the true null of $\tau=.5,\beta=2$ no
more than 5\% of the time at $\alpha=.05$ --- the plots are darkest in the
area where the two lines showing the true parameters intersect. All of the
plots have some power to reject non-true alternatives --- as we can see with
the large white areas in all of the plots. However, only when we add
information about the number of treated neighbors to the SSR+Degree
statistic, do we see high power against all alternatives in the plane.


\begin{figure}[h!]
\centering
\includegraphics[width=.99\textwidth]{twoDplots.pdf}
\caption{Proportion of $p$-values less than .05 for tests of joint hypotheses
about $\tau$ and $\beta$. Darker values mean rare rejection. White means
rejection always. Truth is shown at the intersection of the straight
lines  $\tau=.5, \beta=2$. Each panel shows a different test statistic. The
SSR Tests refer to eq, the KS tests refer to eq.  }\label{fig:twoD}
\end{figure}

The bottom row of Figure~\ref{fig:twoD} demonstrates the power of the KS test.
The bottom right hand panel shows the test used in BFP. Again all of the tests
are valid in the sense of rejecting the truth no more than 5\% of the time
when $\alpha=.05$ although all of these tests are conservative: the SSR based
tests rejected the truth roughly 4\% of the 10,000 simulations but the KS
tests rejected the truth roughly 2\% of the time.  The KS+Design and
KS+Degree panels show the power of applying the KS test to residuals from linear
models including network degree only (the +Degree version) or degree and also
the number of treated neighbors (a quantity that changes for each simulation
of the experiment). That is, where as the SSR panels used the sum of squared
residuals after accounting for network degree and/or number of treated
neighbors, the KS+Design and KS+Degree panels apply the KS test to the raw
residuals. These panels show (1) that inclusion of a quantity from the true
model (number of treated neighbors) is not enough to increase power against
all alternatives to the level shown by the SSR+Design test statistic and (2)
that the KS tests and the SSR tests have different patterns of power --- the
KS tests might be less powerful in general (more darker areas on the plots),
but our general sense from the analysis accords with the general sense about
the KS test in the scholarly literature --- that it is relatively low powered
(cites).

We cannot say here whether the SSR+Design test will provide the best power
against relevant alternatives for all possible models of treatment effect
propagation and designs. However, we hope that this research note both
improves the application of the BFP approach and raises new questions for
research.  Via counterexample, we can see that effect
increasing test statistics are not always appropriate for assessing hypotheses
about interference. BFP is correct in the assertion that, regardless of the
choice of test statistic selection, a set of implausible hypotheses is
identified by the procedure. But we should not be led to believe that, for any
given test statistic, that some hypotheses are more plausible than others.
Such inferences -- comparing hypotheses -- may depend on the test statistic
used, and not necessarily reflect the plausibility of the model at hand.


